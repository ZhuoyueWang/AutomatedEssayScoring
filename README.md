#Automate Essay Scoring System

This system is inspired by the Kaggle competition:
https://www.kaggle.com/c/asap-aes/data


Automated essay scoring system is the use of specialized computer programs to assign grades
to essays written in an educational setting. It is a method of educational assessment and an
application of natural language processing. Its objective is to classify a large set of textual
entities into a small number of discrete categories, corresponding to the possible gradesâ€”for
example, the numbers 1 to 6. Therefore, it can be considered a problem of statistical
classification based on the grammar check and the possible topic distribution.


Files:
    Dataset:
        Set1Complete.csv: Provided by Kaggle. It's a persuasive GRE Essay dataset, with 1,785 essays and resolved score range 2-12
        perfect.csv: Full point essays from Set1Complete.csv (original dataset), generated by goldenStandard.py

    Execution Program:
        goldenStandard.py: Get full point essays from the original dataset, which set the golden standard for evaluating all essays
        stage1.py: Statistical Analysis Module
        stage2.py: Syntax Analysis Module
        stage3.py: Spell Checker & Syntactic Error Checker
        stage4.py: Semantic Analysis Module
        stage5.py: Discourse Analysis Module
        scoring_model.Rmd: Run statistical models to fit scores in five modules to predict the final score
        test.py: Run all five modules and apply model in scoring_model.Rmd to get the final score evaluation

    Result:
        scores.csv: The evaluation score in each module and the actual score generated by test.py
        scoring_model.html: The report about some statistical models which fits the score data in scores.csv
        result.txt: The prediction score and the actual score.


Required package:
    NLTK, numpy, sklearn, scipy, SpellChecker, Enchant
    RStudio and related package


How to run:
    1. Get to the project folder and open your terminal
    2. Run "python goldenStandard.py" to get perfect.csv
    3. Run "python test.py -d perfect.csv" to get scores.csv (In fact we get scores.csv and result.txt at the same time)
    4. Run "scoring_model.Rmd" to get the best predicted scoring model and its weight
    5. Modified the weight in test.py and Run "python test.py -d perfect.csv" again to get the result.txt (The weight has been set, so you can get scores.csv and result.txt at the same time, like I described in step 3)


PS: The step to run "python test.py -d perfect.csv" takes around 120 minutes.
